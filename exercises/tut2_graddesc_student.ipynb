{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd02e0c5",
   "metadata": {},
   "source": [
    "# Tutorial 2: Neural Network Training\n",
    "The tutorial is meant to build some understanding of how neural networks learn relationships between features `x` and a \n",
    "target `y`. The tutorial deals with gradient descent. In the exercises, we assume that students are familiar with the general form of neural networks. Hence, the architecture of neural networks is not part of the exercises. Instead, we recommend going through \"Ex04-NN-Primer-part1.ipynb\". That notebook covers the architecture of neural networks and the learning procedure. Here, we only focus on the latter.\n",
    "\n",
    "\"Ex04-NN-Primer-part1.ipynb\" treats topics like gradient descent for neural networks of general architecture. For this exercise, we restrict the architecture of the considered neural networks to the form\n",
    "$f(x)=\\beta\\cdot sigmoid(x)$. This corresponds to a very simple neural network with linear output function, sigmoid activation, 1 hidden layer and bias (constants) forced to zero. By considering this simple neural network, the code becomes simpler, and you can (hopefully) gain a better intuition of neural network learning procedures.  \n",
    "\n",
    "We will go through further exercises covering back-propagation and stochastic gradient descent during the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27cd65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b724d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "## create data\n",
    "x = np.array(range(-10,10))\n",
    "y = 2*sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4026fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cebc36f5",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Our goal is to find the coefficient beta, such that the function $f(x)=\\beta\\cdot sigmoid(x)$ fits the data best \n",
    "(according to the mean squared error).  \n",
    "Your task is to implement gradient descent in order to find beta. This means in detail:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ade5fe6",
   "metadata": {},
   "source": [
    "\n",
    "### Part A.\n",
    "You need to calculate the derivative of the loss function $L(Y,f(X))=\\frac{1}{n}\\sum_{i}(y_{i}-f(x_{i}))^{2}$ w.r.t. $\\beta$. For simplicity we provide an impementation of this loss function called `grad_beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d877efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_f(beta, x):\n",
    "    return beta*sigmoid(x)\n",
    "def grad_beta(beta, y, x): \n",
    "    return np.mean(-2*(y-func_f(beta, x))*sigmoid(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d8fc3ac",
   "metadata": {},
   "source": [
    "### Part B. \n",
    "Implement a function `grad_desc(beta_ini, lrate, n_epochs)`, with an initial value of beta, the learning rate and the number of iterations (called epochs) as parameters. The function should find the $\\beta$ leading to the minimum loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2748e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(beta_ini, lrate, n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        grad_beta_i = grad_beta(beta_ini, y, x)\n",
    "        beta_ini = beta_ini - lrate * grad_beta_i\n",
    "    return beta_ini"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e01de131",
   "metadata": {},
   "source": [
    "### Part C. \n",
    "Apply your function for `beta_ini=0`, `n_epochs=20` and some learning rates of your choice. Which is the best learning rate? What happens for particularly high or low learning rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22120a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x\n",
    "y_train = func_f(4, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2725b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.100, Beta_hat: 1.662, MSE: 2.324\n",
      "Learning rate: 0.001, Beta_hat: 0.034, MSE: 6.686\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.001]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    beta_hat = grad_desc(0, lr, 20)\n",
    "    mse = (np.square(y_train - beta_hat * sigmoid(x_train))).mean()\n",
    "    print(f\"Learning rate: {lr:.3f}, Beta_hat: {beta_hat:.3f}, MSE: {mse:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
